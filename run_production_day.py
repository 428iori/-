# -*- coding: utf-8 -*-
"""
AIæ ªå¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆç¿Œå¯„ä»˜ãç´„å®šç‰ˆãƒ»æ—¥æ¬¡CSVãƒ­ã‚°ä»˜ããƒ»å£²å´ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
- æ–°è¦è²·ã„ / å£²å´ ã¯åŸå‰‡ã€Œç¿Œå–å¼•æ—¥ã®å§‹å€¤ï¼ˆopenï¼‰ã€ã§ç´„å®š
- SLIPPAGE ã‚’å°å…¥ï¼ˆè²·ã„ã¯ 1+SLIPPAGEã€å£²ã‚Šã¯ 1-SLIPPAGEï¼‰
- å£²å´æ™‚ã«å½“æ—¥/éå»ã®ä¾¡æ ¼ãŒç„¡ã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§åˆ©ç”¨
- equity_state.json / equity_log.csv ã‚’æ›´æ–°
"""
import numpy as np
import pandas as pd
import yfinance as yf
import lightgbm as lgb
import warnings, json, os, datetime, requests
from pathlib import Path
from sklearn.isotonic import IsotonicRegression
from sklearn.linear_model import LogisticRegression
warnings.filterwarnings("ignore")

# =========================
# è¨­å®šï¼ˆæ±ºå®šç‰ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
# =========================
START = "2015-01-01"
END   = None
RANDOM_SEED = 42
TRAIN_MONTHS = 6
TOP_K = 3
INIT_CAPITAL = 1_000_000
PER_POS_FRAC = 0.20
COMMISSION   = 0.0005

USE_CALIBRATION = True
CALIB_METHOD = "isotonic"
FIXED_SCORE_QUANTILE = 0.50
SOFTMAX_TEMP = 0.15
RET_CLIP_LOW, RET_CLIP_HIGH = -0.08, 0.30

# SLIPPAGE: ç´„å®šã§ã®ä¾¡æ ¼æ‚ªåŒ–ï¼ˆä¾‹ 0.0002 = 0.02%ï¼‰
SLIPPAGE = 0.0002

# CSV ãƒ­ã‚°è¨­å®š
ENABLE_CSV_LOG = True
LOG_FILE = Path("equity_log.csv")

# Discord
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")
STATE_FILE = Path("equity_state.json")

ALL_TICKERS = [
    "AAPL","MSFT","GOOGL","AMZN","META","NVDA","TSLA","AMD","NFLFX","ADBE",
    "CRM","INTC","IBM","ORCL","QCOM","AVGO","CSCO","TXN","MU","SHOP",
    "SNOW","PANW","TEAM","DDOG","PLTR","UBER","ABNB","PYPL","NOW",
    "ZM","CRWD","MDB","RBLX","NET","ZS","COST","WMT","HD","LOW",
    "TGT","MCD","SBUX","NKE","KO","PEP","PG","PM","DIS","BKNG",
    "F","GM","CAT","BA","GE","DE","UPS","FDX","HON","MMM",
    "LMT","RTX","NOC","GD","XOM","CVX","COP","PSX","MPC","OXY",
    "SLB","EOG","DVN","APA","FCX","NEM","JPM","BAC","C","MS",
    "GS","BLK","SCHW","SPGI","ICE","V","MA","AXP","JNJ","PFE",
    "MRK","BMY","LLY","UNH","CI","HUM","CVS","TMO","DHR","MDT"
]

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ï¼ˆç‰¹å¾´é‡ç­‰ï¼‰
# =========================
def rsi(series, n=14):
    diff = series.diff()
    up = diff.clip(lower=0)
    dn = -diff.clip(upper=0)
    ma_up = up.ewm(alpha=1/n, adjust=False).mean()
    ma_dn = dn.ewm(alpha=1/n, adjust=False).mean()
    rs = ma_up / (ma_dn + 1e-12)
    return 100 - (100 / (1 + rs))

def make_features(df, ticker):
    c, v = df["close"], df["volume"]
    df["sma_s"] = c.rolling(10).mean()
    df["sma_m"] = c.rolling(30).mean()
    df["sma_l"] = c.rolling(90).mean()
    df["rsi"] = rsi(c, 14)
    df["ret1"] = c.pct_change()
    df["ret5"] = c.pct_change(5)
    df["volatility"] = c.pct_change().rolling(20).std()
    df["ma_gap"] = (c - df["sma_m"]) / (df["sma_m"] + 1e-12)
    df["ticker"] = ticker
    return df.dropna().reset_index(drop=True)

FEATS = ["sma_s","sma_m","sma_l","rsi","ret1","ret5","volatility","ma_gap"]

def load_all_data_fast(tickers):
    print(f"Downloading {len(tickers)} tickers...")
    data = yf.download(tickers, start=START, end=END, auto_adjust=True, group_by='ticker', threads=True, progress=False)
    out = []
    for t in tickers:
        try:
            if isinstance(data.columns, pd.MultiIndex):
                if t not in data.columns.levels[0]:
                    print(f"âš  ãƒ‡ãƒ¼ã‚¿ç„¡ã—: {t}")
                    continue
                df = data[t].copy().reset_index()
            else:
                df = data.copy().reset_index()
            df.columns = [c.lower() for c in df.columns]
            need = {"date","open","high","low","close","volume"}
            if not need.issubset(set(df.columns)):
                print(f"âš  æ¬ æåˆ—: {t}")
                continue
            feat = make_features(df, t)
            out.append(feat)
        except Exception as e:
            print(f"âš  {t}: {e}")
            continue
    if not out:
        raise RuntimeError("No data fetched.")
    all_df = pd.concat(out, ignore_index=True).sort_values(["date","ticker"]).reset_index(drop=True)
    print(f"å…¨ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(all_df)}; æœŸé–“: {all_df['date'].min().date()} ï½ {all_df['date'].max().date()}")
    return all_df

def add_future_labels_inplace(df):
    for n in [1,3,5]:
        df[f"ret_next{n}"] = df.groupby("ticker")["close"].shift(-n) / df["close"] - 1
        df[f"label{n}"] = (df[f"ret_next{n}"] > 0).astype(int)
    return df

def train_models(df):
    params = {
        "objective":"binary","metric":"auc","random_state":RANDOM_SEED,
        "learning_rate":0.05,"num_leaves":31,"n_estimators":400,"verbosity":-1
    }
    models = {}
    for k in ["1","3","5"]:
        d = df.dropna(subset=[f"label{k}"])
        if d.empty:
            models[k] = None
            continue
        X, y = d[FEATS], d[f"label{k}"]
        models[k] = lgb.train(params, lgb.Dataset(X, label=y))
    return models

def predict_models(models, df):
    X = df[FEATS]
    out = {}
    for k in ["1","3","5"]:
        if models.get(k) is None:
            out[f"p{k}"] = np.zeros(len(df))
        else:
            out[f"p{k}"] = models[k].predict(X)
    return out

def estimate_mu(df):
    mu = {}
    for k in ["1","3","5"]:
        pos = df[f"ret_next{k}"].loc[df[f"label{k}"]==1]
        mu[k] = pos.mean() if len(pos)>0 else 0.0
    return mu

# =========================
# çŠ¶æ…‹ç®¡ç† / Discord / CSV logging
# =========================
def load_state():
    if not STATE_FILE.exists():
        return {"capital": INIT_CAPITAL, "positions": []}
    return json.load(open(STATE_FILE, "r", encoding="utf-8"))

def save_state(state):
    json.dump(state, open(STATE_FILE, "w", encoding="utf-8"), indent=2, ensure_ascii=False)

def notify_discord(msg):
    if not DISCORD_WEBHOOK_URL:
        print(msg); return
    try:
        requests.post(DISCORD_WEBHOOK_URL, json={"content": msg})
    except Exception as e:
        print(f"Discordé€šçŸ¥å¤±æ•—: {e}")

def append_daily_log(run_date_jst, market_date, realized, unrealized, equity, capital, positions, buys, sells):
    if not ENABLE_CSV_LOG:
        return
    pos_summary = []
    for p in positions:
        pos_summary.append(f"{p['ticker']}:{p['hold_days']}/{p.get('held_days',0)}:{int(p['amount'])}")
    pos_summary_str = ";".join(pos_summary)
    buys_str = ";".join(buys) if buys else ""
    sells_str = ";".join(sells) if sells else ""

    prev_equity = None
    if LOG_FILE.exists():
        try:
            prev = pd.read_csv(LOG_FILE)
            if len(prev)>0:
                prev_equity = float(prev.iloc[-1]["equity"])
        except Exception:
            prev_equity = None

    if prev_equity is None or prev_equity == 0:
        daily_ret = 0.0
    else:
        daily_ret = (equity / prev_equity - 1.0) * 100.0

    row = {
        "run_date_jst": run_date_jst.isoformat(),
        "market_date": pd.to_datetime(market_date).date().isoformat(),
        "realized_pnl": float(realized),
        "unrealized_pnl": float(unrealized),
        "equity": float(equity),
        "capital": float(capital),
        "num_positions": len(positions),
        "positions": pos_summary_str,
        "buys": buys_str,
        "sells": sells_str,
        "daily_return_pct": float(daily_ret)
    }

    dfrow = pd.DataFrame([row])
    header = not LOG_FILE.exists()
    dfrow.to_csv(LOG_FILE, mode="a", index=False, header=header)

# =========================
# å£²å´ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼
# =========================
def get_last_available_price(all_df_local, ticker, date):
    sub = all_df_local[(all_df_local["ticker"]==ticker) & (all_df_local["date"]<=date)].sort_values("date", ascending=False)
    if not sub.empty:
        return float(sub.iloc[0]["close"])
    return None

# =========================
# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç¿Œå¯„ä»˜ãç´„å®šãƒ­ã‚¸ãƒƒã‚¯ï¼‰
# =========================
def simulate_continuous(all_df):
    df = all_df.copy()
    df["date"] = pd.to_datetime(df["date"])
    unique_dates = sorted(df["date"].unique())
    market_date = df["date"].max()           # ã‚·ã‚°ãƒŠãƒ«ç®—å‡ºã«ä½¿ã†æœ€æ–°å¸‚å ´ãƒ‡ãƒ¼ã‚¿æ—¥
    # next trading date (for next-open execution), if exists
    try:
        idx_today = unique_dates.index(pd.Timestamp(market_date))
        next_date = unique_dates[idx_today + 1] if idx_today + 1 < len(unique_dates) else None
    except ValueError:
        next_date = None

    tr = df[df["date"] >= market_date - pd.DateOffset(months=6)]
    te = df[df["date"] == market_date]
    df_next = df[df["date"] == next_date] if next_date is not None else pd.DataFrame(columns=df.columns)

    if tr.empty or te.empty:
        print("âš  ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        return

    # ãƒãƒƒãƒ—åŒ–ï¼ˆé«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰
    te_close_map = {r.ticker: float(r.close) for r in te.itertuples()}
    te_open_map = {r.ticker: float(r.open) for r in te.itertuples()}
    next_open_map = {r.ticker: float(r.open) for r in df_next.itertuples()} if not df_next.empty else {}
    next_close_map = {r.ticker: float(r.close) for r in df_next.itertuples()} if not df_next.empty else {}

    state = load_state()
    capital = state.get("capital", INIT_CAPITAL)
    old_positions = state.get("positions", [])
    realized_pnl = 0.0
    sold_lines = []
    sells_for_log = []
    sells_skipped_for_log = []
    remaining = []

    # ---------- å£²å´å‡¦ç†ï¼ˆä¿æœ‰æº€äº†â†’åŸå‰‡ next_open ã§ç´„å®šã€ãªã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ ----------
    for pos in old_positions:
        pos["held_days"] = pos.get("held_days", 0) + 1
        if pos["held_days"] >= pos["hold_days"]:
            sell_price = None
            sell_source = None
            # å„ªå…ˆï¼šç¿Œå–¶æ¥­æ—¥ã®å§‹å€¤ï¼ˆnext_openï¼‰
            if next_date is not None and pos["ticker"] in next_open_map:
                sell_price = next_open_map[pos["ticker"]] * (1.0 - SLIPPAGE)
                sell_source = f"next_open({next_date.date()})"
            # æ¬¡ã®å„ªå…ˆï¼šå½“æ—¥ã®çµ‚å€¤ï¼ˆte_closeï¼‰
            elif pos["ticker"] in te_close_map:
                sell_price = te_close_map[pos["ticker"]] * (1.0 - SLIPPAGE)
                sell_source = f"te_close({market_date.date()})"
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´è¿‘åˆ©ç”¨å¯èƒ½ãª close ã‚’ä½¿ã†
                last_price = get_last_available_price(df, pos["ticker"], market_date)
                if last_price is not None:
                    sell_price = last_price * (1.0 - SLIPPAGE)
                    sell_source = "last_available_before_or_on_market_date"
                else:
                    sell_price = None
                    sell_source = "no_price_found"

            if sell_price is None:
                # ä¾¡æ ¼ãŒç„¡ãã¦å£²å´ã§ããªã„ â†’ ä¿æœ‰ç¶™ç¶š
                remaining.append(pos)
                sells_skipped_for_log.append(f"{pos['ticker']}:skip_no_price")
                continue

            # åˆ©ç›Šè¨ˆç®—ï¼ˆæ‰‹æ•°æ–™ã‚’è€ƒæ…®ï¼‰
            shares = pos["amount"] / pos["buy_price"]
            proceeds = shares * sell_price
            buy_comm = COMMISSION * pos["amount"]
            sell_comm = COMMISSION * proceeds
            profit = proceeds - pos["amount"] - buy_comm - sell_comm

            realized_pnl += profit
            sold_lines.append(f"{pos['ticker']} ({pos['hold_days']}æ—¥): {((sell_price/pos['buy_price']-1)*100):+.2f}% ({profit:+,.0f}å††) [{sell_source}]")
            sells_for_log.append(f"{pos['ticker']}:profit={profit:+.0f}:src={sell_source}")
        else:
            remaining.append(pos)

    # ã‚¹ã‚­ãƒƒãƒ—ãƒ­ã‚°ã‚’çµ±åˆ
    if sells_skipped_for_log:
        sells_for_log.extend(sells_skipped_for_log)

    capital += realized_pnl

    # ---------- æ–°è¦è³¼å…¥ï¼ˆã‚·ã‚°ãƒŠãƒ«ã«å¯¾ã— next_open ã‚’å„ªå…ˆã—ã¦ç´„å®šä¾¡æ ¼ã‚’è¨­å®šï¼‰ ----------
    add_future_labels_inplace(tr)
    add_future_labels_inplace(te)
    models = train_models(tr)
    mu = estimate_mu(tr)
    preds = predict_models(models, te)

    scores = np.vstack([preds["p1"]*mu["1"], preds["p3"]*mu["3"], preds["p5"]*mu["5"]]).T
    best_score = scores.max(axis=1)
    te["score"] = best_score
    thr = np.quantile(best_score, FIXED_SCORE_QUANTILE)
    buys_df = te[te["score"] >= thr].sort_values("score", ascending=False).head(TOP_K)

    new_positions = []
    buys_for_log = []
    if not buys_df.empty:
        per_trade = capital * PER_POS_FRAC / len(buys_df)
        for i, row in enumerate(buys_df.itertuples()):
            p1, p3, p5 = preds["p1"][i], preds["p3"][i], preds["p5"][i]
            idx = np.argmax([p1*mu["1"], p3*mu["3"], p5*mu["5"]])
            hold_days = [1,3,5][idx]

            # ç´„å®šä¾¡æ ¼ã®æ±ºå®šï¼ˆå„ªå…ˆ: next_open -> å½“æ—¥çµ‚å€¤ -> ç›´è¿‘ availableï¼‰
            buy_price = None
            buy_source = None
            if next_date is not None and row.ticker in next_open_map:
                buy_price = next_open_map[row.ticker] * (1.0 + SLIPPAGE)
                buy_source = f"next_open({next_date.date()})"
            elif row.ticker in te_close_map:
                buy_price = te_close_map[row.ticker] * (1.0 + SLIPPAGE)
                buy_source = f"te_close({market_date.date()})"
            else:
                last_price = get_last_available_price(df, row.ticker, market_date)
                if last_price is not None:
                    buy_price = last_price * (1.0 + SLIPPAGE)
                    buy_source = "last_available_before_or_on_market_date"
                else:
                    # ä¾¡æ ¼ãŒå–å¾—ã§ããªã„éŠ˜æŸ„ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®‰å…¨ï¼‰
                    continue

            new_positions.append({
                "ticker": row.ticker,
                "buy_price": float(buy_price),
                "amount": float(per_trade),
                "hold_days": int(hold_days),
                "held_days": 0
            })
            buys_for_log.append(f"{row.ticker}:hold={hold_days}:price={buy_price:.2f}:src={buy_source}")

    all_positions = remaining + new_positions

    # å«ã¿è©•ä¾¡ï¼ˆè©•ä¾¡ã«ã¯ market_date ã®çµ‚å€¤ã‚’ç”¨ã„ã‚‹ã€‚å¿…è¦ãªã‚‰ next_close_map ã«å¤‰æ›´å¯èƒ½ï¼‰
    unrealized = 0.0
    for pos in all_positions:
        cur_price = te_close_map.get(pos["ticker"], pos["buy_price"])
        unrealized += (cur_price / pos["buy_price"] - 1.0) * pos["amount"]

    current_equity = capital + unrealized

    # çŠ¶æ…‹ä¿å­˜ï¼ˆcapital ã¯ç¢ºå®šæ®‹é«˜ï¼‰
    state = {"capital": capital, "positions": all_positions}
    save_state(state)

    # é€šçŸ¥ä½œæˆï¼ˆJSTãƒ™ãƒ¼ã‚¹ã®å®Ÿè¡Œæ—¥ï¼‰
    run_date_jst = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).date()
    sold_str = "\n".join(sold_lines) if sold_lines else "ï¼ˆå£²å´ãªã—ï¼‰"
    buy_str = "\n".join([f"{p['ticker']} ({p['hold_days']}æ—¥ä¿æœ‰) @ {p['buy_price']:.2f}" for p in new_positions]) if new_positions else "ï¼ˆæ–°è¦è³¼å…¥ãªã—ï¼‰"

    msg = (
        f"ğŸ“… **{run_date_jst} ãƒˆãƒ¬ãƒ¼ãƒ‰çµæœ**\n"
        f"**å¸‚å ´ãƒ‡ãƒ¼ã‚¿æ—¥:** {market_date.date()}\n"
        f"**ç´„å®šæƒ³å®šæ—¥ï¼ˆnext trading dateï¼‰:** {next_date.date() if next_date is not None else 'N/A'}\n"
        f"**å£²å´:**\n{sold_str}\n"
        f"**æ–°è¦è³¼å…¥:**\n{buy_str}\n"
        f"**å®Ÿç¾æç›Š:** {realized_pnl:+,.0f}å††\n"
        f"**å«ã¿æç›Š:** {unrealized:+,.0f}å††\n"
        f"**è©•ä¾¡æ®‹é«˜ï¼ˆå«ã¿è¾¼ã¿ï¼‰:** {current_equity:,.0f}å††\n"
        f"**ç¢ºå®šæ®‹é«˜ï¼ˆç¾é‡‘ï¼‰:** {capital:,.0f}å††"
    )
    print(msg)
    notify_discord(msg)

    # CSV ãƒ­ã‚°æ›¸ãè¾¼ã¿
    append_daily_log(run_date_jst=run_date_jst,
                     market_date=market_date,
                     realized=realized_pnl,
                     unrealized=unrealized,
                     equity=current_equity,
                     capital=capital,
                     positions=all_positions,
                     buys=buys_for_log,
                     sells=sells_for_log)

# =========================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆåœŸæ—¥ã‚¹ã‚­ãƒƒãƒ—ï¼‰
# =========================
def main():
    # åœŸæ—¥ã‚¹ã‚­ãƒƒãƒ—ï¼ˆJSTãƒ™ãƒ¼ã‚¹ï¼‰
    weekday = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).weekday()  # æœˆ=0, æ—¥=6
    if weekday >= 5:
        msg = f"ğŸ›Œ {datetime.date.today()} ã¯ä¼‘å ´æ—¥ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—"
        print(msg)
        notify_discord(msg)
        return

    all_df = load_all_data_fast(ALL_TICKERS)
    simulate_continuous(all_df)

if __name__ == "__main__":
    main()














